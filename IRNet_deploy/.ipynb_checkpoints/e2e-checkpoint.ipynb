{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src import args as arg\n",
    "from src import utils\n",
    "from src.models.model import IRNet\n",
    "from src.rule import semQL\n",
    "import argparse\n",
    "import sem2SQL\n",
    "import copy\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import tempfile\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    action_embed_size=128,\n",
    "    att_vec_size=300,\n",
    "    batch_size=64,\n",
    "    beam_size=10,\n",
    "    clip_grad=5.0,\n",
    "    col_embed_size=300,\n",
    "    column_att='affine',\n",
    "    column_pointer=True,\n",
    "    cuda=True,\n",
    "    dataset='./fdata',\n",
    "    decode_max_time_step=40,\n",
    "    dropout=0.3,\n",
    "    embed_size=300,\n",
    "    epoch=50,\n",
    "    glove_embed_path='./data/glove.42B.300d.txt',\n",
    "    hidden_size=300,\n",
    "    load_model='./saved_model/IRNet_pretrained.model',\n",
    "    loss_epoch_threshold=50,\n",
    "    lr=0.001,\n",
    "    lr_scheduler=True,\n",
    "    lr_scheduler_gammar=0.5,\n",
    "    lstm='lstm',\n",
    "    max_epoch=-1,\n",
    "    model_name='rnn',\n",
    "    no_query_vec_to_action_map=False,\n",
    "    optimizer='Adam',\n",
    "    query_vec_to_action_diff_map=False,\n",
    "    readout='identity',\n",
    "    save='save',\n",
    "    save_to='model',\n",
    "    seed=90,\n",
    "    sentence_features=True,\n",
    "    sketch_loss_coefficient=1.0,\n",
    "    toy=False,\n",
    "    type_embed_size=128,\n",
    "    word_dropout=0.2\n",
    ")\n",
    "\n",
    "def load_data(tmpdir):\n",
    "    global table_data, sql_data, schemas\n",
    "    \n",
    "    import copy\n",
    "    cargs = copy.deepcopy(args)\n",
    "    cargs.dataset = os.path.join(tmpdir, 'fdata')\n",
    "    \n",
    "    _, table_data, sql_data, _ = utils.load_dataset(cargs.dataset, use_small=args.toy)\n",
    "    with open(os.path.join(args.dataset, 'tables.json'), 'r', encoding='utf8') as f:\n",
    "        table_datas = json.load(f)\n",
    "    schemas = dict()\n",
    "    for i in range(len(table_datas)):\n",
    "        schemas[table_datas[i]['db_id']] = table_datas[i]\n",
    "\n",
    "        \n",
    "def load_model():\n",
    "    global model\n",
    "    \n",
    "    grammar = semQL.Grammar()\n",
    "    model = IRNet(args, grammar)\n",
    "    if args.cuda: model.cuda()\n",
    "    print('load pretrained model from %s'% (args.load_model))\n",
    "    pretrained_model = torch.load(args.load_model,\n",
    "                                     map_location=lambda storage, loc: storage)\n",
    "    pretrained_modeled = copy.deepcopy(pretrained_model)\n",
    "    for k in pretrained_model.keys():\n",
    "        if k not in model.state_dict().keys():\n",
    "            del pretrained_modeled[k]\n",
    "    model.load_state_dict(pretrained_modeled)\n",
    "    model.word_emb = utils.load_word_emb(args.glove_embed_path)\n",
    "\n",
    "\n",
    "def feed_question(qstr, db_id='concert_singer'):\n",
    "    template = json.load(open(\"preprocess/fuck.json\"))[0]\n",
    "    template['db_id'] = db_id\n",
    "    template['question'] = qstr\n",
    "    template['question_toks'] = nltk.tokenize.TweetTokenizer().tokenize(qstr)\n",
    "    \n",
    "    tempdir = tempfile.mkdtemp('lalala')\n",
    "    print('cp -r fdata ' + tempdir)\n",
    "    os.system('cp -r fdata ' + tempdir)\n",
    "    json.dump([template], open(os.path.join(tempdir, \"tmp.json\"), \"w\"))\n",
    "    \n",
    "    print('preprocess/run_me.sh {} tables.json {}'.format(os.path.join(tempdir, \"tmp.json\"), os.path.join(tempdir, \"fdata\", \"dev.json\")))\n",
    "    os.system('preprocess/run_me.sh {} tables.json {}'.format(os.path.join(tempdir, \"tmp.json\"), os.path.join(tempdir, \"fdata\", \"dev.json\")))\n",
    "    return tempdir\n",
    "\n",
    "    \n",
    "def do_prediction(db_id='concert_singer'):\n",
    "    print(sql_data)\n",
    "    perm = list(range(len(sql_data)))\n",
    "    examples = utils.to_batch_seq(sql_data, table_data, perm, 0, 1, is_train=False)\n",
    "    example=examples[0]\n",
    "    print(example.src_sent)\n",
    "    results = model.parse(example, args.beam_size)[0]\n",
    "    sql_sentences = []\n",
    "    for result in results:\n",
    "        modified_example = {'model_result_replace': ' '.join([str(x) for x in results[0].actions]), **sql_data[0]}\n",
    "        sql_sentence = sem2SQL.transform(modified_example, schemas[db_id])[0]\n",
    "        sql_sentences.append(sql_sentence)\n",
    "    print(sql_sentences)\n",
    "    return sql_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Column Pointer:  True\n",
      "load pretrained model from ./saved_model/IRNet_pretrained.model\n",
      "Loading word embedding from ./data/glove.42B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "# load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2e_demo(qstr, db_id='concert_singer'):\n",
    "    tempdir = feed_question(qstr, db_id=db_id)\n",
    "    load_data(tempdir)\n",
    "    return do_prediction(db_id=db_id)\n",
    "\n",
    "def e2e_execute_demo(qstr, db_id='concert_singer', last=None):\n",
    "    import sqlalchemy\n",
    "    import pandas as pd\n",
    "    sqls = e2e_demo(qstr, db_id=db_id)\n",
    "    \n",
    "    for sql in sqls:\n",
    "        if last is not None and last != '':\n",
    "            sql = sql.replace('= 1', \"= '{}'\".format(last))\n",
    "            print(sql)\n",
    "        joins = list(re.findall('([^\\s]+)\\s+AS\\s+([^\\s]+)\\s+JOIN\\s+([^\\s]+)\\s+AS\\s+([^\\s]+)', sql))\n",
    "        ssql = re.split('[^\\s]+\\s+AS\\s+[^\\s]+\\s+JOIN\\s+[^\\s]+\\s+AS\\s+[^\\s]+', sql)\n",
    "        print(ssql, joins)\n",
    "        assert len(ssql) == len(joins) + 1\n",
    "        nsql = ''\n",
    "        \n",
    "        dbschema = schemas[db_id]\n",
    "        column_names = dbschema['column_names_original']\n",
    "        table_names = dbschema['table_names_original']\n",
    "        fk = dbschema['foreign_keys']\n",
    "        \n",
    "        def get_common_key(t1, t2):\n",
    "            i1 = table_names.index(t1)\n",
    "            i2 = table_names.index(t2)\n",
    "            columns_1 = [(i, y) for i, (x, y) in enumerate(column_names) if x == i1]\n",
    "            columns_2 = [(i, y) for i, (x, y) in enumerate(column_names) if x == i2]\n",
    "            for _, x in columns_1:\n",
    "                for _, y in columns_2:\n",
    "                    if x == y: return x, y\n",
    "            return None, None\n",
    "            \n",
    "        \n",
    "        for x in range(len(ssql)):\n",
    "            nsql += ssql[x]\n",
    "            if x != len(ssql) - 1:\n",
    "                print(joins[x])\n",
    "                k1, k2 = get_common_key(joins[x][0], joins[x][2])\n",
    "                if k1 is not None:\n",
    "                    nsql += '{0} AS {1} JOIN {2} AS {3} ON {1}.{4} = {3}.{5}'.format(*(list(joins[x]) + [k1, k2]))\n",
    "                else:\n",
    "                    nsql += '{0} AS {1} JOIN {2} AS {3}'.format(*joins[x])\n",
    "        sql = nsql\n",
    "        \n",
    "        print(nsql)\n",
    "        engine = sqlalchemy.engine.create_engine(\"sqlite:///../text2sql/database/{}/{}.sqlite\".format(db_id, db_id))\n",
    "        df = pd.read_sql(sql, engine)\n",
    "        if df.shape[0] != 0: return sql, df\n",
    "    \n",
    "    return sql, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "e2e_execute_demo(\"Give me the author of the cheapest book\", db_id=\"book_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_execute_demo(\"show me the titles of the books\", db_id=\"book_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import request, jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/query')\n",
    "def query():\n",
    "    q = request.args.get('q')\n",
    "    db = request.args.get('db')\n",
    "    last = request.args.get('last', '')\n",
    "    \n",
    "    sql, val = e2e_execute_demo(q, db_id=db, last=last)\n",
    "    try:\n",
    "        return jsonify({\n",
    "            \"data\": json.loads(val.to_json(orient='records')),\n",
    "            \"sql\": sql\n",
    "        })\n",
    "    except:\n",
    "        return jsonify({\n",
    "            \"data\": [],\n",
    "            \"sql\": ''\n",
    "        })\n",
    "\n",
    "app.run(host='0.0.0.0', port=443)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36_torch120] *",
   "language": "python",
   "name": "conda-env-py36_torch120-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
