{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2333)\n",
    "torch.cuda.manual_seed(2333)\n",
    "np.random.seed(2333)\n",
    "random.seed(2333)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = json.load(open(\"data/tables.json\"))\n",
    "train_other = json.load(open(\"data/train_others.json\"))\n",
    "train_spider = json.load(open(\"data/train_spider.json\"))\n",
    "dev = json.load(open(\"data/dev.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_size = 128\n",
    "query_size = 380\n",
    "bert_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(x, size):\n",
    "    if len(x) < size:\n",
    "        return x + [0] * (size - len(x))\n",
    "    return x[:size - 1] + [102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_sen = {}\n",
    "for d in dbs:\n",
    "    s = \"[CLS] \" + d[\"db_id\"] + \" [SEP]\"\n",
    "    for i in range(len(d[\"table_names\"])):\n",
    "        t = d[\"table_names\"][i]\n",
    "        s += \" \" + t\n",
    "        for j, c in d[\"column_names\"]:\n",
    "            if j == i:\n",
    "                s += \" \" + c\n",
    "        s += \" [SEP]\"\n",
    "    DB_sen[d[\"db_id\"]] = s\n",
    "DB_tok = {k: align(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(v)), DB_size) for k, v in DB_sen.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/python3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-3a1301004576>\", line 10, in <module>\n",
      "    neg.append((q_tok, v))\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/python3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/python3.6/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/anaconda3/envs/python3.6/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/python3.6/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/anaconda3/envs/python3.6/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/anaconda3/envs/python3.6/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/anaconda3/envs/python3.6/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/anaconda3/envs/python3.6/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/opt/anaconda3/envs/python3.6/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/opt/anaconda3/envs/python3.6/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/opt/anaconda3/envs/python3.6/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "pos, neg = [], []\n",
    "for train_data in [train_other, train_spider, dev]:\n",
    "    for i in range(len(train_data)):\n",
    "        q = \"[CLS] \" + train_data[i][\"question\"] + \" [SEP]\"\n",
    "        q_tok = align(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(q)), query_size)\n",
    "        for k, v in DB_tok.items():\n",
    "            if k == train_data[i][\"db_id\"]:\n",
    "                pos.append((q_tok, v))\n",
    "            else:\n",
    "                neg.append((q_tok, v))\n",
    "# pos = random.sample(pos, 1000)\n",
    "neg_sam = random.sample(neg, len(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pos + neg_sam\n",
    "y = [1] * len(pos) + [0] * len(neg_sam)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, device):\n",
    "        # self.Xq = torch.tensor([x[0] for x in X]).to(device)\n",
    "        self.Xd = torch.tensor([x[0] + x[1] for x in X]).to(device)\n",
    "        self.Xt = torch.tensor([ [0] * len(x[0]) + [1] * len(x[1]) for x in X]).to(device)\n",
    "        self.y = torch.Tensor(y).to(device)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.Xd[index], self.Xt[index]), self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_data = MyDataset(X_train, y_train, device)\n",
    "train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "val_data = MyDataset(X_val, y_val, device)\n",
    "val_iter = torch.utils.data.DataLoader(val_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.MD = nn.ModuleDict({\n",
    "            \"encoder\": BertModel.from_pretrained('bert-base-uncased'),\n",
    "            # \"query_encoder\": BertModel.from_pretrained('bert-base-uncased'),\n",
    "            # \"db_encoder\": BertModel.from_pretrained('bert-base-uncased'),\n",
    "            \"linear1\": nn.Linear(bert_size, 768),\n",
    "            \"linear2\": nn.Linear(768, 300),\n",
    "            \"linear3\": nn.Linear(300, 1)\n",
    "        })\n",
    "        \n",
    "        for submodel in [self.MD[\"encoder\"]]:\n",
    "            for param in submodel.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        db, tok = x\n",
    "        x = self.MD['encoder'](db, token_type_ids=tok)\n",
    "        # Q = self.MD[\"query_encoder\"](query)\n",
    "        # D = self.MD[\"db_encoder\"](db)\n",
    "#         x = torch.sum(Q[0][:, 0, :] * D[0][:, 0, :], axis=-1)\n",
    "#         print(x)\n",
    "        # x = torch.cat([Q[0][:, 0, :], D[0][:, 0, :]], -1)\n",
    "        x = torch.nn.functional.relu(self.MD[\"linear1\"](x[0][:, 0, :]))\n",
    "        x = torch.nn.functional.relu(self.MD[\"linear2\"](x))\n",
    "        x = self.MD[\"linear3\"](x)\n",
    "        return torch.sigmoid(x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "loss = nn.BCELoss()\n",
    "model = net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    model.train()\n",
    "    l_sum, acc, n = 0.0, 0, 0\n",
    "    for X, y in train_iter:\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        l_sum += l.item() * y.shape[0]\n",
    "        acc += torch.sum((y_pred > 0.5) == (y == 1)).item()\n",
    "        n += y.shape[0]\n",
    "    model.eval()\n",
    "    l_sum_val, acc_val, n_val = 0.0, 0, 0\n",
    "    for X, y in val_iter:\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        l_sum_val += l.item() * y.shape[0]\n",
    "        acc_val += torch.sum((y_pred > 0.5) == (y == 1)).item()\n",
    "        n_val += y.shape[0]\n",
    "    val_loss = l_sum_val / n_val\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"save/best_model.pt\")\n",
    "    print(\"epoch\", epoch, \", train acc:\", acc / n, \", train loss:\", l_sum / n, \", val acc:\", acc_val / n_val,  \", val loss:\", val_loss, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
